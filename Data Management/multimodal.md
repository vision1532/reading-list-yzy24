## Multimodal Data Management
| Paper | Conference | Remark |
| :---:| :---:| :---:|
|[Learned Data-aware Image Representations of Line Charts for Similarity Search](https://dl.acm.org/doi/pdf/10.1145/3588942)|sigmod2023|一种从数据相似度和图像相似度两个层面检索line chart的方法。作者研究的是在只提供image而不提供数据的情景下进行相似查询。提出LineNet模型，一个基于vision transformer的三元自编码器，目的是将linechart的chart学习为与其data相关联的一种表征，以至于在每次query时获得表征并在embedding space中进行近邻搜索，精确区分chart背后的data distribution。三个难点：a.设计一个能够同时捕捉chart特点和数据分布进而学习表征的模型；b.没有足够的chart-data对来支撑模型的训练；c.如何得到有代表性的训练样本。为解决b作者自己创建了一个110K的chart-data数据集，并众包了带标签的进行评估。LineNet的训练是有监督的，需要带有相似/不相似标签的charts，也就是训练样本的问题c，作者提出伪标签机制，将chart的data用来做distance计算，并设定阈值，低于便是similar，这样便得到了data aware image similarity。经过伪标签的计算后就得到了三元组：anchor，positive，negative，对应三个自编码器，训练目标是降低正样本与anchor的距离，提升负样本与anchor的距离。部署LineNet时，将查询图片编码再近邻查询就可|
|[DATACOMP:In search of the next generation of multimodal datasets](https://proceedings.neurips.cc/paper_files/paper/2023/file/56332d41d55ad7ad8024aac625881be7-Paper-Datasets_and_Benchmarks.pdf)|nips2023|
|[Provable Dynamic Fusion for Low-Quality Multimodal Data](https://proceedings.mlr.press/v202/zhang23ar/zhang23ar.pdf)|ICML2023|
|[Multimodal Prompting with Missing Modalities for Visual Recognition](https://arxiv.org/pdf/2303.03369.pdf)|CVPR2023|主要面向解决两个问题：1.数据模态不完整，训练或测试过程中某个模态缺失；2.在计算资源有限的情况下如何在大量参数的模型上微调。作者提出基于prompting的方法以同时解决，设计缺失感知提示为两种形式，集成到预先训练的多模态Transformer中。1）输入级：将提示添加到每层的输入序列中，2）注意级提示学习：用提示修改 MSA 层的输入，将提示拆分为两个具有相同序列长度 的子提示 ，并将它们分别添加到键向量和值向量之前。通过仅训练很少的参数（即提示）来使预训练的Transformer 适应目标域，将缺失模态的不同情况视为不同类型的输入，并采用可学习的提示来减轻模态缺失造成的性能下降。因此，所提出模型的可学习提示的大小可以小于整个Transformer 的 1%，因此与整体微调相比，计算显著减小。|
|[MUST: An Effective and Scalable Framework for Multimodal Search of Target Modality](https://arxiv.org/pdf/2312.06397.pdf)|ICDE2024||
|[Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment](https://aclanthology.org/2023.findings-emnlp.70.pdf)|EMNLP2023|
|[DEMYSTIFYING CLIP DATA](https://arxiv.org/abs/2309.16671)||The paper aims to reveal CLIP's data curation approach and present a transparent algorithm called MetaCLIP to curate high-quality image-text data from raw web data.Metadata plays a central role in mitigating noise and preserving signal.Balancing the distribution is key to maximizing diversity and task-agnostic properties. Sub-string matching acts as an implicit filter to remove noise without manual rules.Curation algorithm enables easy adaptation to new data sources without external filters. MetaCLIP outperforms CLIP's data, showing the effectiveness of the curation approach.clip的结构比较简单，MetaAI团队分析clip的模型效果主要还是依赖于高质量的WIT400M数据集，数据才是决定模型效果的关键而不是模型架构。而且仅提供了有关CLIP如何管理其数据的有限细节，由于缺少复现的关键信息，MetaAI提出了metaclip，目的就是探索如何生成高质量的图像-文本数据集。首先重建CLIP的500k条查询元数据，构造1个entry列表，记录一些常用的词汇，形成metadata，然后对图像-文本对中的文本数据统计text的子字符串在entry的数量，此过程标识包含任何元数据项的文本，有效地将非结构化文本与结构化元数据项关联起来。在500k个条目中，有114k个条目没有匹配项。有16k个entry的text计数占所有text总数的94.5%。这意味着了解训练数据分布的重要性，因为训练数据很可能没有某些视觉概念 ，CLIP管理背后的关键秘密在于平衡匹配每个entry的text数量。利用上述过程，最终得到了400M质量较好的数据集，用这400M训练的CLIP平均指标更好。本文偏向于工程。|
|[MixGen: A New Multi-Modal Data Augmentation](https://arxiv.org/pdf/2206.08358.pdf)|WACV2023| 来自亚马逊李沐团队。提出了 一种用于视觉语言表示学习的联合数据增强方法，能够生成新的图像文本对。MixGen 通过线性插值图像并连接来自两个现有图像-文本对的文本序列来生成新的训练样本，这样可以保留所有的语义信息。该方法在下游视觉语言任务和数据效率上都有绝对提升。关于增强的位置，可以是原始输入层面，也可以是特征嵌入层面，输入级 MixGen 始终比嵌入级 MixGen 表现更好。这个方法是在数据处理的时候做的，即在Data Loader里完成的，无论换什么架构，只要在Data Loader把这一步加进去就行，其他什么都不用改，即插即用能获得普遍的提升。|
|[Towards Semantic Consistency: Dirichlet Energy Driven Robust Multi-Modal Entity Alignment](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2401.17859.pdf)|ICDE2024|
|[Multimodal Graph Learning for Cross-Modal Retrieval](https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch17)|SDM2023|本文研究了图像和文本之间的跨模态检索。研究假设数据集中的每个实例都包含一张图像和一个文本文档。该方法构建一个多模态图，并使用图神经网络为每个节点学习表示。通过构建多模态图，利用图像和文本之间的相似性关系进行相似性传播，探索多模态数据之间的完全相关性，减轻语义的抽象性，学习全面的表示以增强多模态语义一致性。|
