## Large-scale Graph Representation Learning
| Paper | Conference | Remark |
| :---:| :---:| :---:|
|[Transformer for Graphs: An Overview from Architecture Perspective](https://arxiv.org/pdf/2202.08455.pdf)||Transformer for Homogeneous Graph|
|[PGLBox: Multi-GPU Graph Learning Framework for Web-Scale Recommendation](https://xhyccc.github.io/pglbox.pdf)|KDD 2023|[code](https://github.com/PaddlePaddle/PGL/tree/main/apps/PGLBox)|
|[Large-scale graph representation learning with very deep GNNs and self-supervision](https://arxiv.org/pdf/2107.09422.pdf)|KDDCup 2021|[code](https://github.com/google-deepmind/deepmind-research/tree/master/ogb_lsc)|
|[Large-Scale Representation Learning on Graphs via Bootstrapping](https://arxiv.org/pdf/2102.06514.pdf)|ICLR 2022|[code](https://github.com/nerdslab/bgrl)|
|[Pure Transformers are Powerful Graph Learners](https://proceedings.neurips.cc/paper_files/paper/2022/file/5d84236751fe6d25dc06db055a3180b0-Paper-Conference.pdf)|NeurIPS 2022|Tokenized Graph Transformer (TokenGT), token-wise embedding for representing graph information(each node or edge is an independent token), including node identifier and type identifier. Node identifier(Standard orthogonal vectors) can reveal incidence information between two tokens. Type identifier is used to identify whether a token is a node or an edge. [code](https://github.com/jw9730/tokengt)|
|[Sub-graph Contrast for Scalable Self-Supervised Graph Representation Learning](https://arxiv.org/pdf/2009.10273.pdf)|ICDM 2020|[code](https://github.com/yzjiao/Subg-Con)|
|[GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph](https://proceedings.neurips.cc/paper_files/paper/2021/file/f18a6d1cde4b205199de8729a6637b42-Paper.pdf)|NeurIPS 2021|[code](https://github.com/microsoft/GraphFormers)|
|[Pytorch-BigGraph: A Large Scale Graph Embedding System](https://proceedings.mlsys.org/paper_files/paper/2019/file/1eb34d662b67a14e3511d0dfd78669be-Paper.pdf)|MLSys 2019|[code](https://github.com/facebookresearch/PyTorch-BigGraph)|
|[Large-Scale Learnable Graph Convolutional Networks](https://dl.acm.org/doi/pdf/10.1145/3219819.3219947)|KDD 2018|[code](https://github.com/divelab/lgcn/)|
|[Graph Transformer for Graph-to-Sequence Learning](https://arxiv.org/pdf/1911.07470.pdf)|AAAI 2020|句法图学习[code](https://github.com/jcyk/gtos)|
|[Route Planning Using Divide-and-Conquer: a GAT Enhanced Insertion Transformer Approach](https://deliverypdf.ssrn.com/delivery.php?ID=462120099114119086095126072006120067050047004027052090098036102021098053017017123120017010101021043071094126068080106114097074053022074004072005012100127054119098095003065049058123047014018100125039000055065081100067022093025122081094102105031124028083001102075030015015119091064098&EXT=pdf&INDEX=TRUE)|
